{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWkktxZrITEo",
    "outputId": "f732cc02-36e8-48a6-8e8d-5dbe25810101"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#inputs\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "#target outputs\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "#initialize the weights and bias\n",
    "# w = np.zeros(2)\n",
    "w = np.array([0.0, 0.0])\n",
    "b = 0\n",
    "# b = 0\n",
    "learning_rate = 0.1\n",
    "epochs = 100    #iterations\n",
    "\n",
    "#training perceptron\n",
    "for epoch in range(epochs):\n",
    "  for i in range(len(x)):\n",
    "    linear_output = np.dot(x[i], w) + b\n",
    "    prediction = 1 if linear_output >= 0 else 0\n",
    "    error = y[i] - prediction\n",
    "    if error!= 0:\n",
    "      w += learning_rate * error * x[i]\n",
    "      b += learning_rate * error\n",
    "\n",
    "print(\"Final weights:\", w)\n",
    "print(\"Final bias:\", b)\n",
    "\n",
    "print(\"Testing trained perceptron:\")\n",
    "for i in range(len(x)):\n",
    "  linear_output = np.dot(x[i], w) + b\n",
    "  predicted = 1 if linear_output >= 0 else 0\n",
    "  print(f\"Input: {x[i]}, Predicted: {predicted}, True: {y[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnR8LjZIKtbM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
